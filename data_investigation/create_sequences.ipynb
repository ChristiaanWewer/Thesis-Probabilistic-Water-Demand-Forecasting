{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import os\n",
    "from utils import make_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load processed data\n",
    "scaler = torch.load('../data/processed/scaler_all.pt')\n",
    "train_dfs = torch.load('../data/processed/train_dfs.pt')\n",
    "val_dfs = torch.load('../data/processed/val_dfs.pt')\n",
    "test_df = torch.load('../data/processed/test_df.pt')\n",
    "df_naive = (pd.read_csv('../data/processed/naive_model_final.csv', index_col=0, parse_dates=True) - scaler.mean_[:-1])/scaler.scale_[:-1]\n",
    "df_naive_test_probabilistic_lower_bound = (pd.read_csv('../data/processed/naive_test_probabilistic_lower_bound.csv', index_col=0, parse_dates=True) - scaler.mean_[:-1])/scaler.scale_[:-1]\n",
    "df_naive_test_probabilistic_upper_bound = (pd.read_csv('../data/processed/naive_test_probabilistic_upper_bound.csv', index_col=0, parse_dates=True) - scaler.mean_[:-1])/scaler.scale_[:-1]\n",
    "df_naive_train_val_probabilistic_lower_bound = (pd.read_csv('../data/processed/naive_train_val_probabilistic_lower_bound.csv', index_col=0, parse_dates=True) - scaler.mean_[:-1])/scaler.scale_[:-1]\n",
    "df_naive_train_val_probabilistic_upper_bound = (pd.read_csv('../data/processed/naive_train_val_probabilistic_upper_bound.csv', index_col=0, parse_dates=True) - scaler.mean_[:-1])/scaler.scale_[:-1]\n",
    "DMAs = test_df.columns.tolist()[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_val_indexer(indexlist, df, dma):\n",
    "\n",
    "    # function puts the naive model predictions in the tensor format [n, 24], where n is the number of samples\n",
    "    min_index = indexlist[0]\n",
    "    max_index = indexlist[-1]\n",
    "    return torch.Tensor(df.loc[min_index:max_index, dma].copy().to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the one-hot encoding for the DMAs\n",
    "classes = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
    "dict_one_hot = {}\n",
    "for col in DMAs:\n",
    "    col_name = col[4:5]\n",
    "    for c in classes:\n",
    "        if col_name in c:\n",
    "            ind = classes.index(c)\n",
    "            one_hot = np.zeros(len(classes))\n",
    "            one_hot[ind] = 1\n",
    "            dict_one_hot[col] = one_hot\n",
    "df_one_hot = pd.DataFrame(dict_one_hot, index=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing A (1/10)\n",
      "Processing B (2/10)\n",
      "Processing C (3/10)\n",
      "Processing D (4/10)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 94\u001b[0m\n\u001b[1;32m     91\u001b[0m val_indices\u001b[38;5;241m.\u001b[39mextend(val_dma_ind)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# make sequences for the test data\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m test_x, test_y, test_ind \u001b[38;5;241m=\u001b[39m \u001b[43mmake_sequences\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdict_config_sequences\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m num_test_seqs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(test_x))\n\u001b[1;32m    100\u001b[0m test_naive_2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([naive_val_indexer(test_index, df_naive, dma) \u001b[38;5;28;01mfor\u001b[39;00m test_index \u001b[38;5;129;01min\u001b[39;00m test_ind])\n",
      "File \u001b[0;32m/run/media/Christiaan/My Passport/masters_thesis/github/Water Demand Forecasting/data_investigation/utils.py:126\u001b[0m, in \u001b[0;36mmake_sequences\u001b[0;34m(df, historic_sequence_length, prediction_sequence_length, historic_features, future_features, future_one_hots, historic_one_hots, target_feature, static_features, include_historic_target, return_indices, device, dtype)\u001b[0m\n\u001b[1;32m    123\u001b[0m n_historic_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_historic_features)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# if dict_or_tensor == 'dict':\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterate_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_future\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_historic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistoric_sequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_sequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_feature_col_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_one_hots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_one_hots_ind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_unique_one_hots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistoric_one_hots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistoric_one_hots_ind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistoric_unique_one_hots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_historic_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_historic_features_col_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_features_col_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/run/media/Christiaan/My Passport/masters_thesis/github/Water Demand Forecasting/data_investigation/utils.py:210\u001b[0m, in \u001b[0;36miterate_dict\u001b[0;34m(torch_future, torch_historic, torch_index, historic_sequence_length, prediction_sequence_length, target_feature_col_ind, future_features, future_one_hots, future_one_hots_ind, future_unique_one_hots, historic_one_hots, historic_one_hots_ind, historic_unique_one_hots, static_features, n_historic_features, all_historic_features_col_ind, future_features_col_ind, return_indices, n)\u001b[0m\n\u001b[1;32m    207\u001b[0m     x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfuture\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch_future_slice[:,future_features_col_ind] \u001b[38;5;66;03m#torch.tensor(df_future_slice[future_features].values).to(torch.float32)\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m future_one_hots[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, (ind, one_hot_feature) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfuture_one_hots_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_one_hots\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    211\u001b[0m         one_hot_tensor \u001b[38;5;241m=\u001b[39m torch_future_slice[:,ind]\n\u001b[1;32m    212\u001b[0m         one_hot_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mone_hot(one_hot_tensor\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint64), num_classes\u001b[38;5;241m=\u001b[39mfuture_unique_one_hots[j])\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mbool)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# make sequences for each dma, dict version\n",
    "name = '24h_out_all_no_weather'\n",
    "os.makedirs(f'../data/sequences/{name}', exist_ok=True)\n",
    "\n",
    "# placeholder values\n",
    "num_val_seqs = []\n",
    "num_train_seqs = []\n",
    "num_test_seqs = []\n",
    "train_xs, train_ys, train_indices = [], torch.tensor([]), []\n",
    "val_xs, val_ys, val_indices =  [], torch.tensor([]), []\n",
    "test_xs, test_ys, test_indices =  [], torch.tensor([]), []\n",
    "\n",
    "# iterate over the DMAs and turn them into sequences\n",
    "for i, dma in enumerate(DMAs):\n",
    "    dma_name = dma[4:5]\n",
    "    print(f'Processing {dma_name} ({i+1}/{len(DMAs)})')\n",
    "    \n",
    "    static_feature = df_one_hot[dma].to_numpy()\n",
    "\n",
    "    # config of the sequences\n",
    "    dict_config_sequences = {\n",
    "        'historic_sequence_length':     168,\n",
    "        'prediction_sequence_length':   24,\n",
    "        'historic_features':            None,\n",
    "        'future_features':              ['Holidays'],\n",
    "        'future_one_hots':              ['Weekday', 'Hour'],\n",
    "        'historic_one_hots':            ['Weekday', 'Hour'],\n",
    "        'target_feature':               dma,\n",
    "        'static_features':              static_feature,\n",
    "        'include_historic_target':      True,\n",
    "        'return_indices':               True,\n",
    "    }\n",
    "\n",
    "    # make placeholders\n",
    "    train_dma_x, train_dma_y, train_dma_ind =  [],  torch.tensor([]), []\n",
    "    num_train_s_in_loop = []\n",
    "\n",
    "    # make sequences for each of the splits that contain the training data\n",
    "    for train_df in train_dfs:\n",
    "        train_x, train_y, train_ind = make_sequences(\n",
    "            train_df,\n",
    "            **dict_config_sequences\n",
    "            )\n",
    "               \n",
    "        train_dma_x.extend(train_x)\n",
    "        train_dma_y = torch.cat((train_dma_y, train_y))\n",
    "        train_dma_ind.extend(train_ind)\n",
    "        num_train_s_in_loop.append(len(train_x))\n",
    "    num_train_seqs.append(num_train_s_in_loop)\n",
    "    \n",
    "    # save train sequences\n",
    "    torch.save(train_dma_x, f'../data/sequences/{name}/train_x_{name}_{dma_name}.pt')\n",
    "    torch.save(train_dma_y, f'../data/sequences/{name}/train_y_{name}_{dma_name}.pt')\n",
    "    torch.save(train_dma_ind, f'../data/sequences/{name}/train_ind_{name}_{dma_name}.pt')\n",
    "\n",
    "    train_xs.extend(train_dma_x)\n",
    "    train_ys = torch.cat((train_ys, train_dma_y))\n",
    "    train_indices.extend(train_dma_ind)\n",
    "\n",
    "    # make sequences for each of the splits that contain the validation data\n",
    "    val_dma_x, val_dma_y, val_dma_ind = [],  torch.tensor([]), []   \n",
    "    num_val_s_in_loop = []\n",
    "    for val_df in val_dfs:\n",
    "        val_x, val_y, val_ind = make_sequences(\n",
    "            val_df,\n",
    "            **dict_config_sequences\n",
    "            )\n",
    "\n",
    "        val_dma_x.extend(val_x)\n",
    "        val_dma_y = torch.cat((val_dma_y, val_y))\n",
    "        val_dma_ind.extend(val_ind)\n",
    "        num_val_s_in_loop.append(len(val_x))\n",
    "\n",
    "    num_val_seqs.append(num_val_s_in_loop)\n",
    "    val_dma_naive2 = torch.stack([naive_val_indexer(test_index, df_naive, dma) for test_index in val_dma_ind])\n",
    "    val_dma_naive_up = torch.stack([naive_val_indexer(test_index, df_naive_train_val_probabilistic_upper_bound, dma) for test_index in val_dma_ind])\n",
    "    val_dma_naive_low = torch.stack([naive_val_indexer(test_index, df_naive_train_val_probabilistic_lower_bound, dma) for test_index in val_dma_ind])\n",
    "\n",
    "    # save val sequences\n",
    "    torch.save(val_dma_x, f'../data/sequences/{name}/val_x_{name}_{dma_name}.pt')\n",
    "    torch.save(val_dma_y, f'../data/sequences/{name}/val_y_{name}_{dma_name}.pt')\n",
    "    torch.save(val_dma_ind, f'../data/sequences/{name}/val_ind_{name}_{dma_name}.pt')\n",
    "    torch.save(val_dma_naive2, f'../data/sequences/{name}/val_naive_final_{name}_{dma_name}.pt')\n",
    "    torch.save(val_dma_naive_up, f'../data/sequences/{name}/val_naive_up_final_{name}_{dma_name}.pt')\n",
    "    torch.save(val_dma_naive_low, f'../data/sequences/{name}/val_naive_low_final_{name}_{dma_name}.pt')\n",
    "\n",
    "    val_xs.extend(val_dma_x)\n",
    "    val_ys = torch.cat((val_ys, val_dma_y))\n",
    "    val_indices.extend(val_dma_ind)\n",
    "\n",
    "    # make sequences for the test data\n",
    "    test_x, test_y, test_ind = make_sequences(\n",
    "        test_df,\n",
    "        **dict_config_sequences\n",
    "        )\n",
    "    num_test_seqs.append(len(test_x))\n",
    "    \n",
    "    test_naive_2 = torch.stack([naive_val_indexer(test_index, df_naive, dma) for test_index in test_ind])\n",
    "    test_naive_up = torch.stack([naive_val_indexer(test_index, df_naive_test_probabilistic_upper_bound, dma) for test_index in test_ind])\n",
    "    test_naive_low = torch.stack([naive_val_indexer(test_index, df_naive_test_probabilistic_lower_bound, dma) for test_index in test_ind])\n",
    "\n",
    "    # save test sequences\n",
    "    torch.save(test_x, f'../data/sequences/{name}/test_x_{name}_{dma_name}.pt')\n",
    "    torch.save(test_y, f'../data/sequences/{name}/test_y_{name}_{dma_name}.pt')\n",
    "    torch.save(test_ind, f'../data/sequences/{name}/test_ind_{name}_{dma_name}.pt')\n",
    "    torch.save(test_naive_2, f'../data/sequences/{name}/test_naive_final_{name}_{dma_name}.pt')\n",
    "    torch.save(test_naive_up, f'../data/sequences/{name}/test_naive_up_final_{name}_{dma_name}.pt')\n",
    "    torch.save(test_naive_low, f'../data/sequences/{name}/test_naive_low_final_{name}_{dma_name}.pt')\n",
    "\n",
    "    test_xs.extend(test_x)\n",
    "    test_ys = torch.cat((test_ys, test_y))\n",
    "    test_indices.extend(test_ind)\n",
    "\n",
    "torch.save(train_xs, f'../data/sequences/{name}/train_x_{name}_full_sequence.pt')\n",
    "torch.save(train_ys, f'../data/sequences/{name}/train_y_{name}_full_sequence.pt')\n",
    "torch.save(train_indices, f'../data/sequences/{name}/train_ind_{name}_full_sequence.pt')\n",
    "\n",
    "torch.save(val_xs, f'../data/sequences/{name}/val_x_{name}_full_sequence.pt')\n",
    "torch.save(val_ys, f'../data/sequences/{name}/val_y_{name}_full_sequence.pt')\n",
    "torch.save(val_indices, f'../data/sequences/{name}/val_ind_{name}_full_sequence.pt')\n",
    "\n",
    "torch.save(test_xs, f'../data/sequences/{name}/test_x_{name}_full_sequence.pt')\n",
    "torch.save(test_ys, f'../data/sequences/{name}/test_y_{name}_full_sequence.pt')\n",
    "torch.save(test_indices, f'../data/sequences/{name}/test_ind_{name}_full_sequence.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
